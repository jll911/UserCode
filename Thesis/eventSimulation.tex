\chapter{Event Simulation}
\label{sim}
%somewhere have the pretty picture from 
%matt mc tutorial on how generators/geant/etc 
%fit in with everything else (reco, whatever)

%ARXIV PAPER OF AWESOMENESS
%http://arxiv.org/abs/hep-ph/0403045

%PYTHIA MANUAL OF AWESOMENESS
% the one I'm looking at is lutp0613man2.pdf, 
% but there's probably a more recent one




% I THINK I HAVE TOO MANY LONG SENTENCES 
% AND PREPOSITIONAL PHRASES


%\cite{MCLesHouchesGuide} REFERENCE FOOTNOTE WHATEVER -- DONE BELOW

%need for event simulation: 

Within %the role of % CHANGE THIS CONSTRUCTION
a high-energy physics experiment's role of making 
discoveries and measurements, 
it necessary to know ahead of time what exactly 
should be expected. 
What will the detector signature of a new process look like? 
How will we know whether we've seen something unexpected? 
This is where event simulation contributes.  
%what exactly is the simulated data
Essentially, a series of programs is used to carefully 
generate and calculate all the relevant quantities for a set of 
fake events.  
This information is then used 
to aid analysis in ways for which 
real data alone cannot suffice.  

Having the ability to simulate physics processes 
serves multiple purposes. 
It aids in detector design: 
knowing the expected typical characteristics of particle 
interactions is essential to design a detector 
suited for those interactions.  
Once the detector design is settled, 
simulation of the detector is useful to design 
the algorithms used to reconstruct particles 
from their signature interactions with the 
detector material.  
In addition, simulating the physics processes 
can give estimates of how many 
events of a particular type are expected, 
further aiding the design process.  
The event simulation also contributes 
directly to many analyses, 
in the way of calculating the acceptance, 
the fraction of events that can theoretically 
be detected (see Section~\ref{over:xsec}): 
it is impossible to know from observation 
how many events are missed by the detector, 
because of the very fact that they are unseen.  
It is instead necessary to get that fraction 
from a framework in which 
the characteristics of all events are 
inherently known, not reconstructed.  
Finally, % and possibly most visibly (?) I don't like that
simulated data is directly compared 
with ``real'' data from the detector 
to interpret the real-data results.  
If the data shows something significantly 
different from the simulation, 
then something is missing: 
perhaps a calibration needs to be applied, 
or the response of a detector unit needs 
to be further understood.  
Or, perhaps, there is a new physics process 
appearing for the first time, 
which was not previously known and 
which was therefore not present in the simulation.  
In this case simulation of proposed new physics 
processes may narrow down the identity of the 
observed new process.  
Whatever the case, 
discrepancy between observed data and the event 
simulation indicates that further 
investigation is necessary.  


%points of event simulation: 

%   * directly relevant: compare with data (description of current understanding as is) 
%-- we want to be able to interpret what we're seeing 
%in terms of what physics process it might be

%   * also relevant: acceptance is necessarily a MC calculation!

%   * also: design detectors, design reconstruction and other (e.g. trigger) algorithms

%   * also: event rates (like trigger rates?)  

The entire detection process is simulated, 
including the protons' direct interaction 
and any subsequent particle decays, 
as well as how the end-product particles 
interact with the detector as they pass through 
and how the detector itself responds.  
This latter part includes not only the 
material of the individual subdetectors 
but also the algorithms of 
the Level-1 Trigger, 
which are implemented in hardware 
(see Section~\ref{exp:L1}).  
The High-Level Trigger does not need to be 
simulated in this way; 
since its algorithms are all software-based, 
the same code can be run without modification 
on both real data and simulated data.  



\section{Monte Carlo Event Generation}
\label{sim:MC}

``Event generation'' refers to 
creating a set of 
%fake 
simulated % Wesley
physics events, 
usually realistic, 
for purposes of study and 
comparison with real data.  
The term ``Monte Carlo'' is connected 
with this process 
because at the most fundamental level 
it is based on random numbers.  
All physics interactions happen in 
terms of probabilities: 
process X is more likely to happen than 
process Y, 
so process X is observed more often.  
For any given fake event, 
which physics process is actually simulated 
is randomly determined 
according to those probabilities.  
The kinematics of the event 
(energies and directions of 
involved particles) 
are also assigned according to 
probabilities.  
The final result is that having enough events 
will reproduce the original probabilities 
of each interaction.  
(It is worth noting that in terms of software, 
nothing is really random; in general, 
programs to create ``random numbers''
are really only pseudorandom, 
because computers by nature 
do things according to programmed rules.  
However, for the purpose of event generation, 
pseudorandom numbers are suitable if 
they are sufficiently random-like.  
Moreover, using a deterministic 
pseudorandom generation algorithm 
aids in debugging -- 
the results of the algorithm are reproducible, 
which cannot happen with truly random numbers.)  

%put why called ``monte carlo'', 
%also connect ``event simulation'' terminology 
%with ``generator'' and the multiple types thereof.
%like, ``simulation'' broken down into several steps, 
%one of which is the generation of the main 
%physics process itself (aka ``hard physics process'')
%HOW the events are actually ``generated'', 
%like the whole random number business. 
%It uses a random number generator , 
%to determine ``in a probabilistic way'' -- 
%only one main interaction happens 
%for each proton (bunch) interaction, 
%but the sum over all events ,
%according to the probability of each type of 
%interaction happening.  



%\subsection[Types of Monte Carlo Simulation Programs]{Types of Monte Carlo Simulation Programs\protect\footnote{Material in this section was taken from \cite{MCLesHouchesGuide}.}}  %% DOESN'T WORK!!!! even though it works for chapter headings!
%\subsection{Types of Monte Carlo Simulation Programs\protect\footnote{Material in this section was taken from \cite{MCLesHouchesGuide}.}}
\subsection{Types of Monte Carlo Simulation Programs\footnote{Material in this section was taken from \cite{MCLesHouchesGuide}.}}
\label{sim:MCexplain}

%types of programs with different types of output -- what they're used for. 
%start with tree-level stuff and dsigma eqn...

%   * cross section integrators

%   * event generators

There are two sorts of programs that can be called 
%``Monte Carlo generators''.  
``Monte Carlo event simulators''.  
They both use the matrix elements derived from 
the Feynman diagrams of the physics interactions of interest 
(explained in Section~\ref{theory:feynman}), % THAT MEANS EXPLAIN ALL THIS STUFF IN PREVIOUS CHAPTER
but they use the matrix element formalism 
in different ways to achieve different ends.  
As a reminder, the equation to obtain the 
differential cross section element % WHAT'S IT ACTUALLY CALLED??
from the matrix element for a given diagram is 

%\[
%d\sigma = \frac{1}{\hat{s}}|\mathcal{M}|^2 \frac{d \cos \theta d\phi}{8(2\pi)^2}
%\]
\[
%\frac{d \sigma}{d \Omega} \bigg| _{CM}
d \sigma
= \frac{1}{64 \pi^2 s} \frac{p_f}{p_i} \left| \mathcal{M} \right| ^2 d \Omega
\]

The first type of Monte Carlo program outputs 
a single quantity or set of quantities characteristic 
of the given process, 
such as the cross section.  
These programs are called ``cross section integrators''.  
They calculate 
$d\sigma$ for each of a uniform distribution of 
phase space values (i.e. uniform increments 
of $\cos\theta$ and $\phi$).  
The value of $d\sigma$ represents each event's ``weight''; 
the total cross section $\sigma$ is then obtained 
by integrating over the full collection of $d\sigma$ values.  
The final result is a value for the cross section; 
hence the name ``cross section integrator''.  
However, since the events themselves are distributed 
uniformly in $\cos\theta$ and $\phi$, 
their characteristics 
do not represent actual event distributions 
(real event distributions in data have 
non-uniform distributions, 
in $\cos\theta$ in particular).  
Therefore they can only be used to calculate 
a few select quantities; 
they can't be used to predict distributions.  

The second set of programs are called ``event generators''.  
They also use the matrix element to calculate 
the differential cross section element % REAL NAME ?
with the equation given above.  
However, instead of using $d\sigma$ as an event weight, 
they ``unweight'' the events to obtain the physical 
distributions.  
They do this through the so-called acceptance-rejection method: 
first the maximum possible value of $d\sigma$ is calculated, 
$d\sigma_{max}$. 
Each event's individual $d\sigma$ value, 
the ratio $\frac{d\sigma}{d\sigma_{max}}$ is taken; 
this number is necessarily a fraction between 0 and 1.  
A random number $g$ between 0 and 1 is then generated.  
If the ratio $\frac{d\sigma}{d\sigma_{max}}$ is greater 
than $g$, then the event is kept; %.  
otherwise, it is rejected.  
This method causes the generated events to have the properties 
and distributions that real events would have -- %in reality -- 
sections of phase space with a higher ``event weight'' 
are more likely to be represented.  

%tree-level all well and good, but need corrections to really 
%make things realistic.  
%ways of doing the corrections:

%   * matrix element

%   * parton shower

However, using a single Feynman diagram is not typically 
sufficient to make very accurate calculations.  
%a number or determine distributions is not typically 
%sufficient.  
In general, there are significant contributions from 
the higher-order diagrams  
(diagrams with more legs and loops, 
but still representing the same final arrangement of particles; 
see Section~\ref{theory:HigherOrderDiagrams} 
for more detail on higher-order 
diagrams and calculations). % REF TO EXPLANATION OF PERT SERIES IN THEORY CHAPTER
These corrections must be taken into account.  
Corrections from both QED (photons and electrons) 
and QCD (quarks and gluons) affect the outcome.  
However, due to the strength of the strong force 
(i.e. the high value of the QCD coupling constant), 
QCD has the greater effect %. 
%and calls for .  % what? more effort? more detailed methods?  
and therefore drives the development 
of calculation methods, 
although some of the same methods can also be applied to QED.  
%In addition, the matrix elements and Feynman diagrams   % Wesley: bare quark stuff is low-p stuff, NOT perturbative
%for these calculations only consider bare quarks, 
%that is, quarks extracted from the framework of 
%hadrons such as the proton.  
%These bare quarks cannot exist in reality, 
%so a method of accounting for the quarks' 
%presence inside hadrons must also be used.  
%There have been two different paths taken to deal % path = tack? track?...
%with the above issues.  
There are two complementary approaches commonly in use 
to deal with the corrections.  
In the first, the ``matrix element'' method, 
higher-order diagrams are explicity included in 
the matrix element calculation.  
Since the higher the order, 
the more complex the calculation, 
only diagrams of up to a given order are 
generally used.  
Two approaches exist: 
in one implementation, more ``legs'' can be dealt with, % DO I NEED TO GO INTO ALL THIS DETAIL IF IT'S NOT RELEVANT FOR MY ANALYSIS?
but diagrams containing virtual loops are ignored.  % EXPLAIN IN PREVIOUS CH
In the other, all diagrams, including those with virtual 
loops, of a single order are used, 
but at the moment there exist satisfactory solutions 
only for next-to-leading-order diagrams 
(or NLO, explained in 
Section~\ref{theory:HigherOrderDiagrams}).  
%%only for diagrams one step higher 
%%than the basic tree-level diagram
%The matrix element method does not explicitly 
%take care of the bare-quark problem; %, though.  
%this can be handled in multiple ways.  
%%Programs who do not deal with the interactions of 
%Some cross section integrators deal with the interactions of % is it only cross section integrators??
%quarks inside hadrons using Feyman's factorization theorem. 
%Programs which do not deal with them in this way 
%can be interfaced with programs following the 
%second of the two paths to higher-order corrections, %.  
%%The second path is called 
%the ``parton shower'' method, 
%which is the method used by many event generators 
%(as opposed to cross section integrators).  
%In the parton shower method, 
%instead of including diagrams representing 
%higher orders and quarks-within-hadrons 
%into the matrix element calculation, 
%%this part of the interaction is taken care of 
%corrections are handled 
%by a separate 
%%routine.  
%method.  
%This routine takes the 
In the second of the two approaches, 
the ``parton shower'' method, 
the particles involved in the 
basic tree-level interaction 
%and 
%decays 
%``evolves'' them, 
%through radiation, 
%causes them to radiate 
are made to radiate 
according to 
probabilities and rules coded into the program.  
The process is iterative, 
repeating for any quarks or gluons that 
branch off at any point.  
%In this way the parton shower method 
%is more flexible than the matrix element method -- 
%it can deal with many branchings and legs. 
%However, it is also less precise formally, 
%since it avoids the use of the matrix elements 
%from the Feynman diagrams.  
The matrix element and parton shower methods 
can be used as complements; 
in particular, the parton shower method 
can take output from the matrix element method 
and apply the radiation process.  

%where do the PDFs fit in?  just to parton shower method?? 
%no, it looks like PDFs have to enter in both 
%ways of doing hadronization. 
%(actually, is it only just the initial 
%proton interaction where you need the PDFs?? 
%inside a proton is the only place 
%where x is a relevant quantity...
%dunno, skirt for now)

%do I need more detail at this point?  
%I guess still need to explain modular structure... 
%can more specific stuff like hadronization 
%model go just under pythia, or should it 
%go here?  
%I guess should be more general, 
%since those algorithms are more general.  
%SO, HERE

The philosophy of parton-shower generators 
in particular 
is that the entire process of simulating 
a physics event can be done in a modular fashion.  
Fig.~\ref{fig:MCsteps} 
shows the typical steps necessary to simulate an event.  
The incoming protons are modeled in terms of the 
parton distribution functions (Section~\ref{theory:pdf}).  
The ``hard subprocess'' refers to the main 
physics interaction.  
Not shown in the diagram are other interactions 
involving different parts of the protons 
or other protons in the same bunch; 
these interactions, known as the ``underlying event,''  
are often not as interesting 
%physics-wise % Wesley
because they are typically not as energetic.  
The participant particles in the main interaction 
are then ``evolved'' through the showering process, 
which includes emissions of photons or gluons 
by the initial- or final-state particles, 
as well as ``branchings'' of final particles. % INCLUDED IN FORMER?? 
Any resulting quarks and gluons are 
formed into ``colorless'' hadrons, % but gluons colorless to being with -- and do they ``form'' into hadrsons??
% no, I think gluons are NOT colorless -- you wouldn't be able to get ggg vertices
which may then decay into other particles. 
These final particles are the ones seen 
in the detector.  
The fact that the full process can be separated 
into these steps 
makes it easier for different programs 
to handle individual steps.  
For example, one program may simulate the 
hard process, 
while another is better-equipped to take care 
of the showering and hadronization, 
and a third is specialized in the decay of 
a particular particle.  
%To further the theme of modularity, 
Taking the theme of modularity even further, 
the set of fully-decayed particles 
is then plugged into a program simulating 
the passage of the particles through the detector 
and the detector's response (Section~\ref{sim:Detector}).  

%need to explain since not actually using jets 
%in analysis, but there may be jets in final state
%%(and need to define ``jet'' -- FIXME MORE FOR THEORY SECTION)
%from ISR and FSR
The process of hadronization is not directly 
relevant to this analysis, 
since it does not require hadrons to 
exist in the final state.  
However, since initial- or final-state radiation 
may cause jets incidental to the main interaction 
that nevertheless affect its kinematics, 
hadronization is dealt with here.  
%CAN TREAT ELECTRONS ETC AS ``PARTONS'' AND 
%INCLUDE QED IN SHOWERING -- makes framework easier 
%(maybe put in pythia section?  
%can't remember if it's pythia-specific)
In addition, 
%for initial- or final-state radiation 
%due to photon radiation (QED), 
%it simplifies things to treat 
%electrons and photons in the final state as ``partons'', 
for radiation due to QED, 
electrons and photons in the final state 
can be treated as ``partons'', 
in the same category as the quarks and gluons, 
to simplify the description.  
Even though they are not constituents of the proton, 
they can be modeled as having 
``distribution functions'' 
and subject to ``showers'' of 
photons and electrons. 
%Therefore different sorts of processes can be 
This lets these two very different processes be 
plugged into the same parton shower machinery 
and treated the same way.  

%all this stuff on QCD isn't directly relevant for my 
%stuff, should maybe concentrate more on mine... 
%show the feynman diagram again?  

%parton-shower is default pythia method (for QCD), 
%but matrix element can be used

%And how these two things fulfill the different needs for 
%both higher orders and hadronization

%BIG PRETTY PARTON SHOWER PICTURE but also need to explain PDF part, etc. etc. 
%should compare to other one from zeus people, which one is better? 

 \begin{figure}[htb]
  \begin{center}
    \includegraphics[width=300pt]{Figures/mc-partonShower-lesHouches.png}
  \end{center}
  \caption[\fixspacing Diagram of typical steps used to simulate event in parton shower method]
	  {\fixspacing Diagram of typical steps used to simulate event in parton shower method.
	    The incoming protons are modeled in terms of the 
	    parton distribution functions. 
	    The ``hard subprocess'' refers to the main 
	    physics interaction.  
%	    The participant particles in the main interaction 
%	    are then ``evolved'' through the showering process, 
%	    which includes emissions of photons or gluons 
%	    by the initial or final particles, 
%	    as well as ``branchings'' of final particles. % INCLUDED IN FORMER?? 
%	    Any resulting quarks and gluons are 
%	    formed into ``colorless'' hadrons, 
%	    which may then decay into other particles. 
%	    These final particles are the ones seen 
%	    in the detector.    % commented per Wesley
	  }
  \label{fig:MCsteps}
 \end{figure}


%\subsection{some explanation of the black box?}
%\label{sim:MCBlackBox}
% it's not really a black box anymore!
%can talk about black-box mentality in introduction
%modular structure here?  that pretty picture showing all the 
%different steps to an interaction that need to be 
%simulated. 
%all those steps can be done by separate packages, 
%which is the philosophy.  (pythia manual)
%pdfs ->
%hard scatter (underlying event, multiple interactions in parallel) ->
%[parton] shower (both isr and fsr, incl QED) ->
%hadronization (not directly applicable to my process, but applicable for bg's) -> 
%decay resonances -> interactions with detector material ...

\subsection{Monte Carlo Generator Programs}
\label{sim:MCGens}

%short intro on programs: pythia, powheg, tauola, fewz

%HOW THE PROGRAMS USED TO GENERATE THESE SAMPLES AND NUMBERS 
%FIT INTO THIS FRAMEWORK.  

Several Monte Carlo programs were used to fulfill 
different roles in this analysis.  
Two programs were used to generate the simulation events used: 
one general-purpose generator for background events, 
and a more specialized generator for the signal sample.  
A few of the background samples used a dedicated decay 
program to decay specific particles in the end state.  
Finally, a cross section integrator was used to 
obtain a highly precise estimate of the theoretical 
value of the interaction cross section.  
These programs will be detailed in the following sections.  


%   pythia, powheg, tauola, fewz... else??  (didn't end up using MCatNLO) 
%I don't think anything else.  
%also explain about fewz different (xsec integrator, sounds like)

%I feel like I remember seeing something from herwig?? in one of the 
%production config fragments...

\subsubsection{PYTHIA}
\label{sim:MCGensPythia}

%PYTHIA THE MONSTER

%   * general-purpose parton-shower event generator

%   * pdfs, main process, showering, hadronization, decays

%   * implements many beyond-standard model processes for study

%   * lots of stuff built up over the years, but doesn't do everything


CMS makes use of one of the standard 
Monte Carlo event generation workhorses, 
a program known as PYTHIA (v6.4) 
\cite{PYTHIA6.4}. 
PYTHIA was used to generate the background 
samples used in this analysis, 
with the CTEQ6L1 PDF set \cite{CTEQ6L1}.  
PYTHIA is a general-purpose event generator 
using the parton-shower approach to corrections.  
It encompasses most of the steps necessary to generate 
an event: 
the parton distribution functions 
that describe the colliding protons, 
the main physics interaction process, 
the non-interacting remnants (underlying event)
and how the end products shower, 
hadronize, and decay.  
Over its long history PYTHIA has built up 
a large collection of tools, 
which include many main physics processes and 
various methods of dealing with each of the steps 
mentioned above.  
PYTHIA also includes many non-standard-model 
physics processes 
in order to study new physics theories.  
Most of what it does can be adjusted by the user 
via changing parameters; 
however, its default values are chosen to produce 
valid output without the need to 
necessarily adjust settings.  
Because PYTHIA is such a wide-ranging program, 
it does not treat everything it handles in depth.  
For this reason, specialized programs are sometimes used 
to work with specific areas of the simulation process, 
including the ones in the following sections.  


%and lund string model pictures (both, yay!)
This analysis does not directly deal with 
quarks or jets; 
however, since jets can be present in the final state 
and can play a role in the kinematics of the \Zee 
interaction, 
the method of dealing with hadronization 
%jets and the QCD processes that form them 
is relevant here.  
PYTHIA by default uses the Lund string model 
\cite{LundString} 
%(shown schematically in Fig.~\ref{fig:MClund})
(illustrated in Fig.~\ref{fig:MClund})
to evolve 
quarks and gluons through the process of hadronization 
and formation into jets. %, 
Conceptually, an interaction's final state may have 
quarks 
%and/or gluons 
``flying off in different directions''.  
However, quarks cannot exist alone: %.  % and gluons?
they must always exist with other quarks, 
held together by the strong force.  
If the individual quarks do not have very much energy 
as they move apart, 
the increasing energy of their interaction 
will hold them together.  % EXPLAIN THE ASYMPTOTIC FREEDOM IN THEORY CHAPTER
However, if the quarks have much energy, 
the interaction energy will not be enough to hold 
them together, 
and they will need to form new ``quark partners'' 
in order to 
%not be alone.  
remain in a bound state with other quarks.  
According to the Lund string model, 
%linear confinement -- 
the energy between opposite color charges 
increases approximately linearly with distance. 
The energy is concentrated into a linear area, 
like a tube or string.  
These strings are always stretched between two quarks.  
Enough energy on the part of the quarks will 
``break'' the string and 
create a new quark-antiquark pair, 
which then fills the need for ``quark partners'', 
as illustrated in Fig~\ref{fig:MClundB}.  
The process continues until the quarks 
do not have enough energy to overcome 
the force holding them together.  % wishy-washy use of ``force''?

 \begin{figure}[htb]
  \begin{center}
%    \includegraphics{Figures/mc-partonShower-lesHouches.png}
    \subfloat[]{\label{fig:MClundA}\includegraphics[width=180pt]{Figures/mc-lundStringDiagram-zeusPeople.png}}
    \subfloat[]{\label{fig:MClundB}\includegraphics[width=180pt]{Figures/mc-lundStringModel-zeusPeople.png}}
  \end{center}
  \caption[\fixspacing Schematics showing the Lund string model of hadronization]
	  {\fixspacing Schematics showing the Lund string model of hadronization.
	  \subref{fig:MClundA} In the Lund string model, the quarks 
	  are joined by strings which ``break'' to form new 
	  quark-antiquark pairs. 
	  The process continues until the quarks' energies are  
	  not enough to overcome the force holding them together.  
	  The gray areas represent chains of quarks joined by such strings.  
	  \subref{fig:MClundB} An illustration of a possible 
	  creation of a quark-antiquark pair.  
	  }
  \label{fig:MClund}
 \end{figure}

%pythia is definitely parton shower, 
%explain steps and hadronization model, etc


%what are the different tunes for?  tune z2, tune d6t 
%actually, I think they're for the underlying event

%https://twiki.cern.ch/twiki/bin/view/CMS/PythiaTuning

%so NOT just CMS-based, they're pythia-wide.  
%but I don't see Z2 or D6T on this page: 
%http://home.fnal.gov/~skands/leshouches-plots/

The underlying event 
can be represented in PYTHIA 
by different configurations of parameters, 
known as ``tunes.''  
The tunes have been chosen to correctly model 
already-existing distributions from previous 
experiments.  
This analysis uses samples generated 
according to the default tune used in CMS, 
Tune Z2 \cite{TuneZ2}.  
However, differences between the tunes 
are very slight and affect 
final results very little; 
these affects are accounted for 
in the systematic errors 
(Section~\ref{anMeth:SystsOther}).  

\subsubsection{POWHEG}
\label{sim:MCGensPowheg}
%why is this one used for signal in favor of pythia?  
%more to the point, what exactly is it? 
%and how is it different from pythia? 

%http://www-zeuthen.desy.de/~alioli/talks/Granada.pdf  POWHEG 
%explanatory talk of AWESOMENESS

%% JHEP 0807 (2008) 060, arXiv:0805.4802 
%% http://arxiv.org/abs/0805.4802

POWHEG (for ``POsitive Weight Hardest Emission Generator'') 
\cite{powheg} % VERSION??  I don't think it's actually 1.0, I think that's for the powheg box
is a method for making next-to-leading order (NLO) 
corrections within a parton shower framework.  
At low energies, the showering algorithms implemented 
in parton shower generators (such as PYTHIA) 
model the corrections well.  
However, at higher energies, a next-to-leading order 
matrix calculation is more accurate.  
An ideal solution combines the two approaches 
to get the best of each.  
Since POWHEG does this, 
it therefore produces a more accurate picture 
of the radiation corrections and their effects 
on final-state kinematic quantities 
when compared to a standard parton-shower 
generator, %such as PYTHIA, 
which only includes some (the real part) 
of these corrections.  
POWHEG consists of a number of routines representing 
various %main physics processes, 
interactions, 
including \Zee.  
Processes that are not already included 
in the set may be implemented by the user.  

POWHEG was used to generate the 
hard process for each event 
of the \Zee signal sample, 
using the CT10 PDF set \cite{CT10}; 
this part of the interaction was then 
fed through PYTHIA 
(by the modular procedure previously described) 
to undergo hadronization.  
It was more important for the signal 
sample to be accurate to higher order 
than for the background samples; 
the background samples are only of interest 
in terms of how much they may contaminate 
the signal sample.  
Slight variations in their distributions 
due to missing higher-order corrections are 
therefore not as relevant.  

% don't really need anything else?  
% not from experimentalist point of view!(?)
% experimentalist don't talk about 
% angular-ordered showers


\subsubsection{Other Generators}
\label{sim:MCGensOther}
%Like, whichever other samples I check (background etc) -- actually, they're all pythia

%or just have separate sections for both as for pythia and powheg?  

%TAUOLA AND FEWZ

Two other Monte Carlo event generators 
played roles in the analysis.  
In the first case, 
particles in some of the background samples 
called for a specialized decay package.  
In the second, 
a precise theoretical prediction was needed 
to compare with data, 
and the standard programs used to generate 
the events 
cannot reach the necessary precision.  

%\subsubsubsection{TAUOLA}
%\label{sim:MCGensTauola}

%the first reference for TAUOLA is always
%S. Jadach, J. H. Kuhn, and Z. Was, Comput.
%Phys. Commun. 64 (1990) 275.
% not mentioned in VBTF

% but other refs through the years: 
%http://arxiv.org/abs/hep-ph/0411377
%http://arxiv.org/abs/hep-ph/0610386
%http://arxiv.org/abs/1101.1652

%homepage given as http://wasm.home.cern.ch/wasm/goodies.html
%REAL homepage for any useful stuff is the f77: http://wasm.home.cern.ch/wasm/f77.html
%(c++ page only has interface-specific stuff)

% also abstract included in les houches guidebook -- didn't find it anywhere else

Many particle decays are handled by 
the primary event generator, PYTHIA.  
However, there are several aspects of decays 
that PYTHIA does not deal with, 
such as spin polarization.  
Because these aspects can have small effects 
on the kinematic distributions of the decayed 
particles, 
CMS decided to use a separate package, TAUOLA 
\cite{TAUOLA}, 
to decay tau particles in the samples 
that included taus.  
%While TAUOLA implements non-standard interactions, % NEED THIS?????
%this is not made use of here.  
%Only standard decays are used.  
TAUOLA specifically takes spin polarization 
into account, 
and includes first order QED corrections.  % WHY NOT [need] QCD?? can decay hadronically!
The TAUOLA package was only necessary for the 
\Ztautau and \Wtaunu background samples.  

%\subsubsubsection{FEWZ}
%\label{sim:MCGensFewz}

%AND, FEWZ webpage was on frank petriello's wisconsin space and doesn't exist anymore

%http://www.phys.hawaii.edu/~kirill/FEHiP.htm links to 
%http://www.hep.wisc.edu/~frankjp/FEWZ.html (bad link)

%FEWZ 2.0 abstract on the arXiv: http://arxiv.org/abs/1011.3540 
%from ryan gavin, ye li, frank petriello, and seth quackenbush (15 Nov 2010)

%WHICH VERSION DID VBTF USE??  look at syst AN.  
%e-mail frank about documentation page??

%HA HERE'S THE NEW PAGE: http://gate.hep.anl.gov/fpetriello/FEWZ.html

%https://indico.cern.ch/getFile.py/access?contribId=38\&resId=3\&materialId=slides\&confId=71330  
%tutorial from ryan giving original arXiv references (non-2.0)

%   * arxiv:hep-ph/0603182, arxiv:hep-ph/0609070 -- okay, both of those are calculation papers, not necessarily software papers

%okay, the second one actually mentions fewz. 
%(and the first is only W, so just need second, I think)
%AND, vbtf gives no other references than those two.  
%So I guess that's all there is!

The theoretical prediction for the cross section value 
was obtained with FEWZ \cite{FEWZ2}, 
a cross section integration program 
specifially dealing with production of the W and Z, 
using the MSTW PDF set \cite{MSTW}.  
Previously, all calculations of the cross section 
had been done at next-to-leading order (NLO) in QCD, 
which entails several percent uncertainty.  
However, since the statistical error on 
W and Z results at the LHC 
was quickly expected to reach the order of 1\%, 
it was deemed necessary to undertake 
a calculation at next-to-next-to-leading order (NNLO), 
which gives a theoretical uncertainty of 
approximately the same value.  
(In essence, all higher-order corrections beyond NNLO 
contribute about a percent or less of uncertainty.)  
FEWZ was chosen to calculate the cross sections 
for the CMS analysis 
because of this treatment of NNLO QCD corrections.  


\subsection{Generated Samples}
\label{sim:MCSamples}

The full list of Monte Carlo generated samples 
used for this analysis 
is given in Table~\ref{TableMCSamples}.  
The first line is signal; the rest are background.  

\begin{table}[htbp]
%  \centering
  \begin{center}
    \caption{\fixspacing Monte Carlo samples used in this analysis.}
    \label{TableMCSamples}
%    \begin{tabular}[]{ | l | c | c | }
    \begin{tabular}[]{ | l | c | c | c | c | c | }
      \hline
      Sample & Generator & $n_{events}$ & $\sigma$ (pb) & $\epsilon$ & Scale factor for 36.1\pb \\ \hline \hline
      \Zee & POWHEG & 1998990 & 1666. & 1. & 3.01 $\times 10^{-2}$ \\ \hline  % 0.0300864937
      QCD BCtoE 20-30 & PYTHIA & 2243439 & 235500000. & 0.00056 & 2.12 \\ \hline  % 2.12212946
      QCD BCtoE 30-80 & PYTHIA & 1995502 & 59300000. & 0.00230 & 2.47 \\ \hline  % 2.46738866
      QCD BCtoE 80-170 & PYTHIA & 1043390 & 906000. & 0.0104 & 0.326 \\ \hline  % 0.326003354
      QCD EM 20-30 & PYTHIA & 36920242 & 235500000. & 0.0104 & 2.39 \\ \hline  % 2.39478712
      QCD EM 30-80 & PYTHIA & 71834019 & 59300000. & 0.065 & 1.94 \\ \hline  % 1.93706898
      QCD EM 80-170 & PYTHIA & 8073559 & 906000. & 0.155 & 0.628 \\ \hline  % 0.627916759
      $t\bar{t}$ & PYTHIA & 1099550 & 94.3 & 1. & 3.10 $\times 10^{-3}$ \\ \hline  % 0.0030960211
      \Ztautau & PYTHIA & 2057446 & 1666. & 1. & 2.92 $\times 10^{-2}$ \\ \hline  % 0.0292316785
      \Wenu & PYTHIA & 4856474 & 6153. & 1. & 4.57 $\times 10^{-2}$ \\ \hline  % 0.045737566
      \Wtaunu & PYTHIA & 5207750 & 7899. & 1. & 5.48 $\times 10^{-2}$ \\ \hline  % 0.0547556814
      WW & PYTHIA & 110000 & 2.9 & 1. & 9.52 $\times 10^{-4}$ \\ \hline  % 0.000951727273
      WZ & PYTHIA & 110000 & 0.34 & 1. & 1.12 $\times 10^{-4}$ \\ \hline  % 0.000111581818
      ZZ & PYTHIA & 2113368 & 4.297 & 1. & 7.34 $\times 10^{-5}$ \\ \hline  % 0.0000734002313
    \end{tabular}
  \end{center}
\end{table}

The table also gives the values by which each 
of the samples were scaled in order to be combined.  
Each sample was generated to be a specific size, 
that is, a specific number of events.  
These sizes do not correlate to the same amount 
of real data, though.  
The sample for a common process may correspond 
to a relatively small amount of data, 
while that for a rare process may be equivalent 
to a very large amount of data, %.  
i.e. because of its rarity, a large amount 
of data is required in order to see it.  
These differences are taken into account by 
determining a scale factor for each sample 
according to its size and its cross section 
(which correlates to probability of the interaction).  
This scale factor is used to scale each sample 
to a given luminosity.  
The scale factor must also take into account 
any inefficiency in the generation process.  
For example, if the sample was generated such that 
at least one electron had an energy higher than 10 GeV, 
all the events whose highest-energy electron was 
lower than 10 GeV will be discarded.  
They still count towards the overall cross section, 
though, so they are taken into account by 
an efficiency number, 
essentially the fraction of events that are kept 
in the filtering process.  
The formula for the scale factor $k$ is 
\[
k = \frac{ \mathcal{L} \times \sigma \times \epsilon_{filter} }{n_{events}}
\]
After multiplying each sample by its scale factor, 
samples and event numbers can be added together.  

The QCD samples are broken into smaller 
sections for easier generation: 
three bins were used, 
given in terms of the interaction energy.  
They are also the only samples here 
to need an efficiency factor. 
Since much of what is produced by QCD 
will get cut out by the 
event selection requirements, 
it is useful to pare down the samples 
from the start, 
so they do not take up unnecessary space.  
The QCD samples have filters applied to 
select only events which may be background 
to an analysis using electrons and photons.  
The ``BCtoE'' samples include 
events with electrons 
from the decays of bound states of $b$ and $c$ 
quarks.  
The ``EM'' samples are 
``electromagnetic-enriched,'' 
meaning the generation process applied 
a filter to include electrons and photons, 
as well as objects that look like 
electrons and photons.  
This second group also requires that 
each event not pass the BCtoE filter, 
so that the two samples are mutually exclusive 
and can be combined without 
double-counting.  


\section{Detector Simulation}
\label{sim:Detector}
%\subsection{GEANT Detector Model/Modeling of Particle Interactions in Detector Material}

After the generation of the main physics process, 
and the parton showering and hadronization and decays, 
the role of the detector itself must be taken into account.  
%The final, end-state, decayed particles interact with 
The end-state, decayed particles interact with 
the detector, which causes signals representing 
the particle's position and energy to be recorded as data.  
These must be properly modeled to know, for example, 
%the magnitude of the energy that should be associated 
%with a given signal.  
what sort of signal would result from the passage of a particle 
with a given momentum.  
In addition, the detector itself is not completely passive, 
in that it can change the properties of the 
particles passing through it during the course of measurement.  
Both of these roles necessitate a detailed simulation 
of the detector and its response.  

%ALSO NEED SOMETHING FOR ELECTRONICS, TRIGGER, ETC

\subsection{GEANT Detector Model}
\label{sim:DetectorGeant}

%ha ha, actually talk about geant down here, 
%modeling of matter, 
%interactions of particles with matter

%GEANT the giant

The CMS collaboration uses the software package 
GEANT4 (v9.3) \cite{GEANT4} to model the detector 
within the framework of CMSSW, 
the CMS-specific analysis software 
(see Section~\ref{over:software}).  
GEANT4 provides a toolkit designed 
to accurately represent all facets of 
the detector simulation.  
These include the ability to construct 
a precise geometrical model of the 
detector itself, 
complete with any actual misalignments 
and a model of the detector's magnetic field, 
as well as the physical properties 
of the materials used.  
In addition, the package also handles 
the properties of the 
fundamental particles involved, 
how they behave in the modeled 
magnetic field, 
and how they interact with the different 
materials present in the detector.  
For example, the detection of particles 
in the electromagnetic calorimeter 
depends on those particles causing 
a shower of other particles upon 
interacting with the material of the 
calorimeter; 
this must be modeled.  
Finally, GEANT4 also provides 
the ability to model the response of 
the electronics used to detect the particles, 
for example the response of a photomultiplier tube 
to a light signal caused by the aforementioned shower.  

\subsection{Level-1 Trigger Emulator}
\label{sim:DetectorL1Emul}
%YAY emulator

%talk about how emulator necessary for l1, but not for hlt (duh)

In addition to modeling the detector 
components that do the actual detecting, 
it is necessary to include the response of 
the trigger system (Section~\ref{exp:trigger}), %.  
%This modeling serves several purposes.  
which serves multiple purposes.  
Modeling the trigger system enables 
the estimation of event rates 
for a given luminosity 
before the experiment actually runs 
with that luminosity.  
In particular, 
this permitted the estimation of rates
long before the detector was fully constructed, 
greatly aiding the experiment's 
physics planning.  
In addition, comparison of the real-time 
``online'' performance of the trigger 
with its software counterpart 
can indicate and aid in diagnosing 
problems with the online running.  

The High-Level part of the trigger system (HLT), 
consisting of algorithms implemented in software, 
is simply run ``offline'' as an element 
of the detector simulation.  
However, the Level-1 Trigger (L1), 
being implemented in electronics 
instead of software, 
must be explicitly modeled.  
The sofware package that does this 
is called the Level-1 Trigger Emulator 
\cite{emulator}, 
since it aims to be a bit-by-bit 
emulation of the trigger hardware 
instead of just a simulation of the 
algorithms implemented.  
The L1 Emulator is modular in the 
same sense as the hardware system; 
information is passed between 
separate packages representing the 
different subsystems of the L1.  
Each element of the hardware, 
namely each type of processing card, 
has its own software class to handle 
its own specific functions.  
The software therefore aims to model 
each of the trigger's processing steps 
in accurate detail, 
so that the overall result is identical.  